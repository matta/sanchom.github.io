<!DOCTYPE html>
<!--
This file was rendered by Pollen. Don't edit this file directly. It will be overwritten when Pollen re-renders.
-->
<html>
  <head>
    <!-- Global Site Tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-106592360-1"></script>
    <script>
       window.dataLayer = window.dataLayer || [];
       function gtag(){dataLayer.push(arguments)};
       gtag('js', new Date());

       gtag('config', 'UA-106592360-1');
    </script>
    <meta name="google-site-verification" content="ApapaNT3CEd0OdSE-X9Xy4xF3r_gjtWDR05XS6FANu4" />

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sancho McCann—What is Inception-V2?</title>
    <link href="https://fonts.googleapis.com/css?family=Source+Serif+Pro|Quattrocento|Josefin+Sans|Raleway" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="../site-style.css" />
</head>
  <body>
    <root><h1>What is Inception-v2?</h1><h2>It’s Inception plus Batch Normalization and some 5x5 factoring</h2><p>In <a href="https://arxiv.org/abs/1602.07261">Inception-v4</a>, Szegedy et al. describe Inception-v2:</p><blockquote>“Later the Inception architecture was refined in various ways, first by the introduction of batch normalization (Inception-v2) by Ioffe et al.”</blockquote><p>That paper by Ioffe et al. describes their model like this:</p><blockquote>“The main difference to [Inception-v1] is that the 5 × 5 convolutional layers are replaced by two consecutive layers of 3 × 3 convolutions with up to 128 filters.”</blockquote><h2>It’s an intermediate variant of Inception before -v3, and different than batch-normalized Inception</h2><p><a href="https://arxiv.org/abs/1512.00567">Rethinking the Inception Architecture for Computer Vision</a> has a section (Section 6) titled “Inception-v2”. It describes a network with many more changes than the Batch Normalization paper listed compared to Inception-v1:</p><blockquote>“Here we are connecting the dots from above and propose a new architecture with improved performance on the ILSVRC 2012 classification benchmark. The layout of our network is given in table 1. Note that we have factorized the traditional 7 × 7 convolution into three 3 × 3 convolutions based on the same ideas as described in section 3.1.  For the Inception part of the network, we have 3 traditional inception modules at the 35×35 with 288 filters each. This is reduced to a 17 × 17 grid with 768 filters using the grid reduction technique described in section 5. This is is followed by 5 instances of the factorized inception modules as depicted in figure 5. This is reduced to a 8 × 8 × 1280 grid with the grid reduction technique depicted in figure 10. At the coarsest 8 × 8 level, we have two Inception modules as depicted in figure 6, with a concatenated output filter bank size of 2048 for each tile. The detailed structure of the network, including the sizes of filter banks inside the Inception modules, is given in the supplementary material, given in the <em>model.txt</em> that is in the tar-file of this submission.”</blockquote><p>I can’t find this model.txt file anywhere on the internet, but they present this table describing Inception-v2:</p><p><img src="assets/possibly-inception-v2.png" width="350"/></p><p>“Rethinking Inception” also includes a table that has separate entries for:</p><ul><li>BN-GoogLeNet</li><li>BN-Inception</li><li>Inception-v2</li></ul></root>
  </body>
</html>