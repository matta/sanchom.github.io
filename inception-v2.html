<!DOCTYPE html>
<!--
This file was rendered by Pollen. Don't edit this file directly. It will be overwritten when Pollen re-renders.
-->
<html>
  <head>
    <!-- Global Site Tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-106592360-1"></script>
    <script>
       window.dataLayer = window.dataLayer || [];
       function gtag(){dataLayer.push(arguments)};
       gtag('js', new Date());

       gtag('config', 'UA-106592360-1');
    </script>
    <meta name="google-site-verification" content="ApapaNT3CEd0OdSE-X9Xy4xF3r_gjtWDR05XS6FANu4" />

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sancho McCann—What is Inception-V2?</title>
    <link rel="stylesheet" type="text/css" href="../site-style.css" />
</head>
  <body>
    <root><h1>What is Inception-v2?</h1><h2>It’s Inception plus Batch Normalization and some 5x5 fac­tor­ing</h2><p>In <a href="https://arxiv.org/abs/1602.07261">Inception-v4</a>, Szegedy et al. de­scribe Inception-v2:</p><blockquote>“Later the Inception ar­chi­tec­ture was refined in var­i­ous ways, first by the in­tro­duc­tion of batch nor­mal­iza­tion (Inception-v2) by Ioffe et al.”</blockquote><p>That pa­per by Ioffe et al. de­scribes their mod­el like this:</p><blockquote>“The main difference to [Inception-v1] is that the 5 × 5 con­vo­lu­tion­al lay­ers are re­placed by two con­sec­u­tive lay­ers of 3 × 3 con­vo­lu­tions with up to 128 filters.”</blockquote><h2>It’s an in­ter­me­di­ate vari­ant of Inception be­fore -v3, and different than batch-nor­mal­ized Inception</h2><p><a href="https://arxiv.org/abs/1512.00567">Rethinking the Inception Architecture for Computer Vision</a> has a sec­tion (Section 6) ti­tled “Inception-v2”. It de­scribes a net­work with many more changes than the Batch Normalization pa­per list­ed com­pared to Inception-v1:</p><blockquote>“Here we are con­nect­ing the dots from above and pro­pose a new ar­chi­tec­ture with im­proved per­for­mance on the ILSVRC 2012 classification bench­mark. The lay­out of our net­work is giv­en in ta­ble 1. Note that we have fac­tor­ized the tra­di­tion­al 7 × 7 con­vo­lu­tion into three 3 × 3 con­vo­lu­tions based on the same ideas as de­scribed in sec­tion 3.1.  For the Inception part of the net­work, we have 3 tra­di­tion­al in­cep­tion mod­ules at the 35×35 with 288 filters each. This is re­duced to a 17 × 17 grid with 768 filters us­ing the grid re­duc­tion tech­nique de­scribed in sec­tion 5. This is is fol­lowed by 5 in­stances of the fac­tor­ized in­cep­tion mod­ules as de­pict­ed in figure 5. This is re­duced to a 8 × 8 × 1280 grid with the grid re­duc­tion tech­nique de­pict­ed in figure 10. At the coars­est 8 × 8 lev­el, we have two Inception mod­ules as de­pict­ed in figure 6, with a con­cate­nat­ed out­put filter bank size of 2048 for each tile. The de­tailed struc­ture of the net­work, in­clud­ing the sizes of filter banks in­side the Inception mod­ules, is giv­en in the sup­ple­men­tary ma­te­r­i­al, giv­en in the <em>mod­el.txt</em> that is in the tar-file of this sub­mis­sion.”</blockquote><p>I can’t find this mod­el.txt file any­where on the in­ter­net, but they present this ta­ble de­scrib­ing Inception-v2:</p><p><img src="assets/possibly-inception-v2.png" width="350"/></p><p>“Rethinking Inception” also in­cludes a ta­ble that has sep­a­rate en­tries for:</p><ul><li>BN-GoogLeNet</li><li>BN-Inception</li><li>Inception-v2</li></ul></root>
  </body>
</html>