<html>
  <head>
    <title>Towards Optimal Naive Bayes Nearest Neighbor</title>
    <link href="onbnn.css" rel="stylesheet" type="text/css" />
  </head>
  <body>
    <p>This page outlines my attempt to reproduce the results of Behmo et al.'s
      paper, 'Towards Optimal Naive Bayes Nearest Neighbor'.</p>
    <h4>Discussion</h4>
    <p>Behmo et al. presented the following results in their paper:</p>
    <img src="assets/onbnn-result.png" height="100px" />
    <p>This suggested that the original NBNN method performed poorly when
      run on this 5 class subset of the Caltech 101 dataset. When I attempted
      to reproduce these poor results using the code in experiment_7 below, I
      was unable to produce result that were as poor as they reported. I got
      the following results:</p>
<pre>c101_BACKGROUND_Google: 0.859259 &plusmn; 0.091325
c101_Faces: 0.907407 &plusmn; 0.067179
c101_car_side: 0.992593 &plusmn; 0.020951
c101_airplanes: 0.703704 &plusmn; 0.124171
c101_Motorbikes: 0.877778 &plusmn; 0.102439
Overall: 0.868148 &plusmn; 0.094065</pre>
    <p>After email discussion with Behmo, I discovered they did not implement
      the original NBNN method:</p>
    <img src="assets/original-nbnn.png" height="80px" />
    <p>Instead, they implemented individual, class-specific, binary classifiers
      for this experiment (despite never mentioning "binary" in their
      paper).</p>
    <p>Behmo confirmed with me that the following is their approach to produce
      the number in the Airplanes row, NBNN column, above (34.17 +- 11.35):</p>
    <ul>
    <li>Resize the images in the airplanes, car_side, Faces, Motorbikes, and
    BACKGROUND_Google classes so that the longest side of each image equals 300
    pixels</li>
    <li>Run sample_keypoints.rb on each of those folders to produce the *___.sift
    files</li>
    <li>Load the descriptors from 30 random airplanes sift files as positive training
    data</li>
    <li>Load the descriptors from 30 random sift files from each of car_side, Faces,
    Motorbikes, and BACKGROUND_Google (totaling 120 files) classes together
    into one merged background data set</li>
    <li>Create a binary classifier using the airplanes data as the positive class,
    and the merged data from the other four classes as the negative class</li>
    <li>Test the classifier on 30 random airplane images that don't overlap with the
    training set and record the classifier accuracy</li>
    <li>Repeat 20 times</li>
    </ul>
    <p>So, they were not comparing against NBNN. They were comparing against
      their NBNN-based binary classifier. NBNN performs much better and
      actually very close to their Optimal NBNN correction that they apply to
      their binary classifier.</p>
    <p>With this new information, I implemented the method as Behmo described above. It's
      available below as experiment_8. I got the following results:</p>
<pre>c101_BACKGROUND_Google: 0.088889 &plusmn; 0.073703
c101_Faces: 0.633333 &plusmn; 0.107152
c101_car_side: 0.085185 &plusmn; 0.079522
c101_airplanes: 0.111111 &plusmn; 0.070273
c101_Motorbikes: 0.170370 &plusmn; 0.102372
Overall: 0.217778 &plusmn; 0.210002</pre>
    <p>This was quite worse than the results published in the paper. However,
      the testing methodology described above, in which they just test the
      recall rate on the positive class, seems doubtful. More likely, they
      tested on a 50/50 mix of positive and negative examples, reporting the
      overall classification rate. To check this, we follow the above
      procedure, except we test on 30 random images from the positive class
      along with 30 random images from the four other classes (totally 60
      images, 30 positive, 30 negative). Doing this experiment gives the following
      results:</p>
<pre>c101_BACKGROUND_Google: 0.562319 &plusmn; 0.046422
c101_Faces: 0.805797 &plusmn; 0.050745
c101_car_side: 0.569565 &plusmn; 0.036664
c101_airplanes: 0.569565 &plusmn; 0.043863
c101_Motorbikes: 0.605797 &plusmn; 0.050745
Overall: 0.622609 &plusmn; 0.092847</pre>
    <p>Behmo introduce a complicated optimization procedure to correct for the
      fact that the negative/positive balance in the training set does not
      match the negative/positive balance in the testing set. Instead, we try
      something much simpler: for the distance from a test descriptor to the
      background class, average over the nearest few neighbors rather than
      using the distance to the single nearest neighbor. This gives the
      following results:
<pre>c101_BACKGROUND_Google: 0.928000 &plusmn; 0.054715
c101_Faces: 0.962667 &plusmn; 0.033092
c101_car_side: 0.976000 &plusmn; 0.027520
c101_airplanes: 0.849333 &plusmn; 0.068065
c101_Motorbikes: 0.949333 &plusmn; 0.037854
Overall: 0.933067 &plusmn; 0.044768</pre>
    <p>This actually is better than the optimal NBNN results that Behmo
      reported on this dataset.</p>
    <h4>Code and data</h4>
    <p>The 5-class Caltech 101
      subset: <a href="assets/onbnn-data.tgz">onbnn-data.tgz</a></p>
    <p class="indent">This contains the resized images, the extraction program that Behmo et
      al. used to extract their data, and one example ASCII descriptor file per
      class for comparison. You can extract the descriptors for the images in
      one of the folders by running the command 'ruby sample_keypoints.rb
      [folder]'. The extracted descriptors are the input for the following
      experiments.</p>
    <p>The original NBNN
      method: <a href="assets/experiment_7.tgz">experiment_7.tgz</a></p>
    <p>Behmo et al's version of the original NBNN
      method: <a href="assets/experiment_8.tgz">experiment_8.tgz</a></p>
    <h4>References</h4>
    <ul>
      <li>oNBNN source
        code: <a href="http://code.google.com/p/optimal-nbnn/">optimal-nbnn</a></li>
      <li>Regis Behmo, Paul Marcombes, Arnak Dalalyan, Veronique Prinet,
        <i>Towards Optimal Naive Bayes Nearest Neighbors</i>, European Conference on
        Computer Vision (ECCV 2010)
        [<a href="http://www.minutebutterfly.de/pro/publi/Behmo_Marcombes_Dalalyan_Prinet_ECCV2010.pdf">pdf</a>]</li> 
      <li>O. Boiman, E. Shechtman, and M. Irani. <i>In Defense of Nearest
          Neighbor Based Image Classification</i>. CVPR
        2008. [<a href="http://www.wisdom.weizmann.ac.il/~boiman/publications/InDefenseOfNN.pdf">pdf</a>]</li>
    </ul>
  </body>
</html>
